# Systemantics
by John Gall

## Keywords

- systems, complex, exceptions, failures, bugs, problems, feedback, bypass,
  exploit, reality, malfunction, delusion, fallacy, anergy, expand, unexpected,
  behaves, law, goals, specialization, competition, selection, grow, evolve

## Table of contents

### Part one: basic theory

#### A. The mysterious ways of systems

1. First principles
2. Laws of growth
3. The generalized uncertainty principle
4. A...B..C..Disaster (feedback)
5. The power of positive feedback: a warning
6. What next? (the life cycle of systems)

#### B. Inside and outside

7. The grand illusion
8. Inside systems
9. Delusion systems versus systems delusions
10. Systems-people

#### C. Function and failure

11. Elementary systems-functions
12. Advanced systems-functions
13. The system knows (system goals)
14. Systems-failure (theory of errors)
15. Glitches, gremlins, bugs
16. Form, function, failure
17. Colossal errors
18. Unexpected interactions

#### D. Communication theory

19. Communication theory
20. Information
21. Talking to the system

### Part two: applied systemantics

#### A. systems and self-defense (meta-strategies)

22. How not to solve porblems
23. The Tao of problem avoidance
24. The creative tack

#### B. Practical systems-design

25. Design don'ts
26. Catastrophe theory

#### C. Management and other myths

27. Wishful feedback
28. Fear of feedback
29. Feedback and the future
30. Catalyic managership
31. The "problem" problem
32. The limits to grandiosity
33. Disaster control

#### D. Intervention

34. Where's the problem?
35. Probing the system
36. The problem in the solution
37. Taming systems
38. The problem in the question
39. THe net of Indra
40. Beyond stability
41. After the solution, what? (the next problem)
42. Envoi: beyond expertise
43. Appendix I. Annotated compendium
44. Appendix II. Self-evaluation quizzes
45. Appendix III. Readers' tear-out feedback sheet
46. Appendix IV. horrible examples
47. Appendix V. Glossary
48. Appendix VI. Animal exemplars
49. Appendix VII. List of horrible examples
50. Biased bibliography and good reading list



## 43. Appendix I. Annotated compendium

### PREFACE II:

Cherish your exceptions. Cherish your system-failures.

Cognate Theorem: Cherish your bugs. Study them.

Meta-strategy I: The most effective approach to coping is to learn the basic laws of systems-behavior.

Satir's Summation: Problems are not the problem; coping is the problem.

### Historical Overview

Primal Scenario or Basic Data of Experience: Systems in general work poorly or not at all.

Alternative formulations:

- Nothing complicated works.
- Complicated systems seldom exceed five percent efficiency.
- If anything can go wrong, it will (Murphy's Law)

### Chapter 1: first principles

Fundamental theorem: New systems generate new problems.

Corollary (Occam's Razor): Systems should not be unnecesssarily multipled.

Low of conservation of anergy: The total amount of anergy in the universe is constant.

(Anergy or anergy-state: Any state or condition of the Universe, or any portion of it, that requires the expenditure of human effort or ingenuity to bring it into line with human desires, needs, or pleasures is defined as an ANERGY-STATE. Anergy is measured in units of effort required to bring about the desired change).

Corollary: Systems operate by redistributing anergy into different forms and into accmulations of different sizes.

### Chapter 2: Laws of growth

Systems expand, and as they expand, they encroach

Big-bang theorem:

- Systems tend to expand at 5-6% per annum
- Systems tend to expand to fill the known universe


### Chapter 3: The Generalized Uncertainty Principle

The Generalized Uncertainty Principle (G.U.P.): Systems display antics

Alternatively: Complex systems exhibit unexpected behaior

West's Wisdom: Reality is more complex than it seems

Climax design theorem (non-additivity theorem): A large system produced by expanding the dimensions of a smaller system does not behave like the smaller system.

### Chapter 4: A...B...C..Disaster (feedback)

Le Chatelier's Principle: The system always kicks back

Alternatively:

- Systems get in the way
- Systems tend to oppose their own proper functions

### Chapter 5: The power of positive feedback: a warning

Beware of positive feedback

### Chapter 6: What next? The life cycle of systems:

Systems tend to malfunction conspicuously just after their greatest triumph

Fully prepared for the past (F.P.F.P.):

- The army is now fully prepared to fight the previous war.
- Perfection of planning is the symptom of decay.
- A temporary patch will very likely be permanent.
- The old system is now the new problem.

Alternatively:
The ghost of the old system continues to haunt the new.

### Chapter 7: The Grand Illusion

Functionary's falsity: people in systems do not do what the system says they are doing.

Operational fallacy: the system itself does not do what it says it is doing.

Corollary: the function (or product) is defined by the systems-operations that occur in its performance or manufacture.

Corollary: the larger the system, the less the variety in the product.

A systems-delusion: if Detroit makes it, it must be an automobile.

The naming fallacy: the name is most emphatically not the thing.

### Chapter 8: Inside Systems

The F.L.A.W. (fundamental law of administrative workings): things are what they are reported to be.

Alternative forms of the F.L.A.W.

- the real world is what is reported to the system.
- if it isn't official, it hasn't happened.
- if it didn't happen on camera, it didn't happen.

And conversely: if the system says it happened, it happened.

Corollary #1: a system is no better than its sensory organs.

Corollary #2: to those within a system the outside reality tends to pale and disappear.

Corollary #3: the bigger the system, the narrow and more specialized the interface with individuals.

Harte's Haunting theorem: information rarely leaks up.

Memory joggers:

- the chart is not the patient.
- the dossier is not the person.

### Chapter 9: Delusion systems versus systems delusions

The jet travel paradox: when you get there, you're still not there.

Stein's extension: when you do get there, there's no there there.

Manager's mirage: the system takes the credit (for any favorable eventuality).

### Chapter 10: Systems-people

Systems attract systems-people

Specialized systems select for specialization

Corollary: the end result of extreme competition is bizareness.

Corollary: prolonged selection selects survivers.

Rohe's theorem: designers of systems tend to design ways for themselves to bypass the system.

The explotation theorems: if a system can be exploited, it will be exploited.

### Chapter 11: elementary systems-functions

Basic axiom of systems-function: big systems either work on their own or they don't. If they don't, you can't make them.

Administrator's anxiety: pushing on the system doesn't help.

Corollary: Even trying to be helpful is a delicate and dangerous undertaking.

Corollary: adding manpower to a late software project makes it later.

A simple system may or may not work.

Observation: some complex systems actually function.

Rule of Thumb:

- If a system is working, leave it alone. Don't change anything. (if it ain't broke, don't fix it).
- A complex system that works is invariably found to have evolved from a simple system that worked.
- A complex system designed from scratch never works and cannot be made to work. You have to start over, beginning with a working simple system.


### Chapter 12: advanced system function

Functional indeterminacy Theorem (F.I.T): in complex systems, malfunction and even total non-function may not be detectable for long periods, if ever.

Kantiam theorem: large complex systems are beyond human capacity to evaluate. (Large systems kant be fully known)

Systems law of intertia: a system that performs a certain function or that operates in a certain way will continue to operate in that way regardless of the need or of changed conditions
alternatively: whatever the system has done before, you can be sure it will do it again

Briefly: the system continues to do its thing, regardless of circumstances

### Chapter 13: the system knows (system goals)

Systems develop goals of their own the instant they come into being.

Equifinality: the system is its own best explanation

Alternatively:

- The system is a law unto itself
- Intra-system goals come first
- Systems don't work for you or for me. They work for their own goals.
- The system behaves as if it has a will to live.
- The system behaves as if it has a will of its own.

### Chapter 14: Systems-failure (theory of errors)

Any large system is going to be operating most of the time in failure mode

Fundamental failure theorem (F.F.F.T):

- A system can fail in an infinite number of ways
- The mode of failure of a complex system cannot ordinarily be determined from its structure.


Corollary: The crucial variables are discovered by accident

The fail-safe theorem: when a fail-safe system fails, it fails by failing to fail safe

### Chapter 15: Glitches, gremlins, bugs

If it doesn't fail here, it will fail there.

Glitch-hunter's theorem:

- intermittent failure is the hardest case
- a bug may be purely local, but you and I can never know that for sure
- one does not know all the expected effects of known bugs
- cherish your bugs. Study them.
- error correction is what we do

### Chapter 16: form, function failure

Wiener's wish: the struture of a machine or an organism is an index of the performance that may be expected of it

Emended form:

- form may follow function, but don't count on  it
- new structure implies new functions
- as systems expand, new functions appear suddenly, in step-wise fashion

Specialized incapacity theorem: as systems grow in size and copmlexity, they tend to lose basic functions

### Chapter 17: Colossal errors

Large lumps of liability theorem:

- when big systems fail, the failure is often big
- colossal systems foster collosal errors

Corollary:
colossal errors tend to escape notice

(A system's delusion): if it's treated by doctors it must be a disease

total systems theorems:

- total systems tend to run away (go out of control)
- a total system in a runaway sequence may be forced to grow rapidly or disintegrate in chaos

### Chapter 18: Unexpected interactions

In setting up a new system, tread softly. You may be disturbing another system that is actually working.

### Chapter 19: Communication theory

The inherent limitation:

- experience isn't hereditary. It ain't even contagious.
- The message sent is not necessarily the message received.


Dunn's indeterminacy:

- Every picture tells a story, but not the same story.
- You can't not communicate
- The meaning of a communication is the behavior that results


### Chapter 20: information

Lynd's Lemma: Knowledge for what?

The Basic Information Theorem (B.I.T.): information decays

Whitehead's variation: knowledge does not keep any better than fish

Rate-of-decay theorem: the most urgently needed information decays fastests

Law of interconvertibility: one system's garbage is another system's precious raw material

inaccessibility theorem:
- the information you have is not the information you want
- the information you want is not the information you need
- the information you need is not the informatino you can obtain

Rule of thumb for missing information:
- don't bother to look for it. you won't find it.
- in a closed system, information tends to decrease and hallucination to increase.

### Chapter 21: Talking to the system

Deregulated dinosaur effect: extra brain in tail, tail wags on own schedule

### Chapter 22: How not to solve problems

Inevitability-of-reality fallacy: things have to be the way they are and not otherwise because that's just the way they are.

Unawareness thereom: if you're not aware that you have a problem, how can you call for help?

### Chapter 23: The tao of problem avoidance

If you're not there, the accident can happen without you

Meta-strategy II:

- choose your systems with care.
- Destiny is largely a set of unquestioned assumptions

Peter's creative incompetence theorem: if you obviously can't do it you probably won't be asked.

### Chapter 24: the creative tack

If something isn't working, don't keep doing it. Do somethinge else instead.

Afterthought: do almost anything else.

Meta-strategy III: For maximum success, feel free to switch systems and even to change goals.

### Chapter 25: design don'ts

Do it without a new system if you can

Occam's razor again: avoid unnecessary systems (systems should not be unnecessarily multiplied)

Corollary: do it with an existing system if you can

Corollary: do it with a little system if you can

Agnes Allen's Law: Almost anything is easier to get into than out of

Specificallly:

- taking it down is often more tedious than setting it up.
- avoid unfavorable settings (some things just cna't be done well by a system)

S.L.O.G. Factor (Systems Law of Gravity):
avoid uphill configurations (systems run downhill more easily than uphill)

Alternatively: go with the flow

Inernal friction theorem: loose systems last longer and function better

Corollary: loose systems have larger interstices

Gresham's law: bad design can rarely be overcome by more design, whether good or bad

Spodick's modificatoin: adding numbers to a bad study doesn't clarify it

that is: large amounts of poor data tend to preempt any amount of good data

Brooks' bitter bidding: plan to scrap the first system. You will anyway.

### Chapter 26: Catastrophe Theory

The Jell-O Principle: When everything correlates with everything else, things will never settle down

Brinkley's Breakthrough: togetherness is great, but don't knock get-away-ness

Edsel's edifying admonitio:n: don't put your name on it until you are sure it will float.

### Chapter 27: Wishful feedback

output phobia: output is dagnerous

corollary: keep it in the study phase

subcorollary rule: keep the study under study

wishful feedback theorem: just calling it 'feedback' doesn't mean that it has actually fed back

alternatively: it hasn't fed back until the system changes course

The face-of-the-future theorem: in dealing with the shape of things to come, it pays to be good at recognizing shapes

### Chapter 28: Fear of feedback

The first law of systems-survival: a system that ignores feedback has already begun the process of terminal instability.

### Chapter 29: Feedback and the future

Boulding's Law: Nature is wise only when feedbacks are rapid

(Relativistic law of information transfer): feedback always gives a picture of the past (information travels at finite velocity)

Weinberg's axiom of experience (a pseudodoxy): the future will be like the past, because, in the past, the future was like the past

Gall's emendation: the future is no more predictable now than it was in the past, but you can at least take note of trends

(escape from predestination): when the sytem acts, it participates in the creation of the future. The future is partly determined by what we do now.

### Chapter 30: Catalytic managership

Cataylic managership rule: use the spontaneous offerings of the system

Utilization meta-strategy: utilize the principle of utilization

Vernacular variants: if it's for digging a hole it should probably look something like a shovel. If it looks like a shovel, try using it for digging a hole.

### Chapter 31: The 'problem' problem

Great advances do not come out of systems designed to produce great advances.

Alternatively: complicated systems produce complicated responses to problems.

Ashby's formulation:
complex systems have complex behaviors
major advances take place by fits and starts

### Chapter 32: The limits to grandiosity

The limit theorems:
a. you can't change just one thing
b. you can't change everything
A little grandiosity goes a long way.

Perfectionists's paradox:
in dealing with large systems, the striving for perfection is a serious imperfection.

Alternative formulation:
Perfection can be achieved on the day after the final deadline

Sometimes stated as:
When the current revision is complete, the system will be perfect

Or even as:
The final truth is just around the corner

The rule of thumb (survivor's souffle):
If it's worth doing at all, it's worth doing poorly.

Bateson's Whimsy:
If it's not worth doing, it's worth doing well.


### Chapter 33: Disaster control

Minsky's admonition:
In order to succeed it is necessary to know how to avoid the most likely was to fail.

Jung's runic riddle:
If it puts a weapon in your hand, it is aiming at some kind of violence.

### Chapter 34: Where's the problem?

In order to be effective, an intervention must introduce a change at the correct logical level.

Meta-strategy V:
If a problem seems unsolvable, consider that you may have a meta-problem

### Chapter 35: Probing the system

The law of requisite variety: control is exercised by the element with the greatest variety of behavioral responses

But: probing will get you only so far

In fact: in most cases, you can't get there from here

### Chapter 36: The problem in the solution

The system is altered by the probe used to test it.

In pharmaceutics:the pill that is tested is never consumed (and vice versa)

Addendum: the probe is altered also

Corollary: there can be no system without its observer

Corollary: there can be no observation with outs effects

Pseudodoxy: "if you are not part of the solutionn, you are part of hte problem"

correct form of the above: the solution is often part of the problem

rule: Look for the self-referential point. That's where the problem is likely to be.

more briefly: stay away from self-reference -- this means you.

Pseudodoxy: "this system is the only correct system"

If things seem to be getting worse even faster than usual, consider that the remedy may be at fault.

Translated: stay out of the positive feedback trap

nasal spray axiom:
escalating the wrong solution does not improve the outcome. If things are acting very strangely, consider that you may be in a feedback situation.

Or: when problems don't yield to commonsense solutions, look for the thermostat.


### Chapter 38: The problem in the question
The automatic pilot is not much help with highjackers

If you can't change the sytsem, change the frame - it comes to the same thing

Meta-strategies:

1. the most effective approach to coping is to learn the laws of systems-behavior
2. Choose your systems with care. You don't actually have to join the coast guard
3. For maximum success, feel free to switch systems or even to switch goals.
4. Utilize the principle of utilization
5. If your problem seems unsolvable, consider that you may have a meta-problem


### Chapter 39: The Net of Indra

Any given element of one system is simultaneously an element in an infinity of other systems

Everything correlates.

There is no such thing as noninvolvment.

Or at least:
No noninvolvement here means involvement there.

### Chapter 40: Beyond stabilitiy

The second law of systems-survival:
In order to remain unchanged, the system must change
