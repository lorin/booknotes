# Field Guide to Understanding 'Human Error' by Sidney Dekker

## Table of contents

1. Two views of 'human error'
    * Bad people in safe systems, or well-intentioned people in imperfect
      systems?
    * People do not come to work to do a bad job
    * "Bad apples clearly exist"
    * "Accident-prone" workers
    * Individual differences exist
    * "There has to be some accountability"
    * "The authority-responsibility mismatch"
    * Accountability and the systems approach
    * New models of accountability
    * To report or not to report
    * Accountability and the "just culture"

2. Containing your reactions to failure
3. Doing a 'human error' investigation
4. Explaining the patterns of breakdown
5. Understanding your accident model
6. Creating an effective safety department
7. Building a safety culture
8. Abandoning the fallacy of a quick fix
9. Epilogue: Speaking for the dead


## Callouts

Underneath every simple, obvious story about ‘human error,’ there is a deeper, more complex story about the organization.

If it made sense for people to do what they did, then it may make sense for others as well.

The New View does not claim that people are perfect. But it keeps you from judging and blaming people for not being perfect.

Practitioners are not all exposed to the same kind and level of accident risk. This makes it impossible to compare their accident rates and say that some, because of personal characteristics, are more accident-prone than others.

A “Bad Apple” problem, to the extent that you can prove its existence, is a system problem and a system responsibility.

There is no evidence that a system approach dilutes personal accountability. In fact, second victims show just how much responsibility practitioners take for things that go wrong.

Accountability can mean letting people tell their account, their story.

The challenge is to create a culture of accountability that encourages learning. Every step toward accountability that your organization takes should serve that goal. Every step that doesn’t serve that goal should be avoided.

If you truly want to create accountability and a “just culture” in your
organization, forget buying it off the shelf. It won’t work, independent of how
much you pay for it. You need to realize that it is going to cost you in
different ways than dollars.

The more you react to failure, the less you will understand it.

## Notes

Storytelling is a powerful mechanism for others to learn vicariously from trouble.

1. Don't ask who is responsible, ask what is responsible
2. Link knowledge of the messy details with the creation of justice
3. Explore the potential for restorative justice
4. Go from backward to forward-looking accountability
5. Put second victim support in place


Reactions to failure are typically:
* retrospective
* counterfactual
* judgmental
* proximal


## Important lessons (summarized in chapte r8)

Five sets of reminders about:

* your own org and nature of safety & risk in it
* what to think about when investigating 'human error'
* how to recognize Old View thinking
* how to create progress on safety with the New View

Whenever you try to understand 'human error', do not forget to take the point
of view of the person inside the situation

### Your org & 'human error'

1. Your org is not basically or inherently safe. People have to create safety
   by putting tools & tech to use while negotiating multiple systems goals at
   all levels of your org.
2. The priorities & prefs that people express through their practice may be
   logical reproduction of what entire org finds important
3. Human error is inevitable by-product of pursuit of success in imperfect,
   unstable, resource-constrained world. Occasional human contribution to
   failure occurs because complex systems need an overwhelming human
   contribution for their safety.
4. So 'human error' is never at the root of your safety problems. 'Human error'
   is the effect of trouble deeper inside your system.
5. It also means that 'human error' is not random. 'Human error' is
   systematically connected to features of people's tools, tasks and operating
   environment.


### What to think of when investigating 'human error'

* As far as the people involved were concerned, the otucome was not going to
  happen. Otherwise, they would have done somethign else.
* Nobody comes to work to do a bad job.
* Human error is not the cause of failure, but the effect.
* Explaining one error (e.g., operator error) by pointing to another (e.g., deficient management) does not explain anything.
* To understand what went on in somebody's mind, you have to reconstruct the
  situation in which the mind found itself.
* There is no such thing as the cause of a mishap. What you deem causal depends
  on your accident model.

### Doing something about your 'human error' problem

* No quick fixes
* Reprimanding bad apples accomplishes nothing
* Can't expect employees to be more committed to safety than managers are or
  appear to be
* Problems result from organization's complexity
* Do not expect that you can hold people accountable for their errors if you did not give them enough authority to live up to the responsibility you expect of them.


### Recognizing old view thinking

* Old View thinking sees ‘human error’ as the major threat to basically safe
  systems. Unreliable, erratic people undermine systems of multiple defenses,
  rules, procedures and other safeguards.
* Old View thinking will try to count and categorize errors, and endeavor to
  get the number of ‘human error’ incidents down. It assumes that safety, once
  established, can be maintained by monitoring and keeping people’s performance
  within pre-specified boundaries.
* Old View thinking will (unsuccessfully) try to revert to more automation,
  tighter procedures, closer supervision and reprimands to control erratic
  human performance.
* During downsizing, budget trimming and increased production pressures, Old
  View thinking will misinterpret ‘human errors’ as a source of trouble, when
  they are likely the inevitable downstream consequences of trying to do more
  with less.

### Creating progress on safety with the new view

* To create safety, you don’t need to rid your system of ‘human errors’.
  Instead, you need to realize how people at all levels in the organization
  contribute to the creation of safety and risk through goal trade-offs that
  are legitimate and desirable in their setting.
* Rather than trying to reduce “violations,” New View strategies will find out
  more about the gap between work-as-imagined and work-as-done—why it exists,
  what keeps it in place and how it relates to priorities among organizational
  goals (both stated and unstated).
* New View thinking wants to learn about authority–responsibility
  mismatches—places where you expect responsibility of your people, but where
  their situation is not giving them requisite authority to live up to that
  responsibility.
* You know your organization is maturing toward the New View once it actively
  tries to learn how it is learning about safety. This means your organization
  is calibrating whether its strategies for managing safety and risk are
  up-to-date.
* Every organization has room to improve its safety. What separates a strong
  safety culture from a weak one is not how large this room is. What matters is
  the organization’s willingness to explore this space, to find leverage points
  to learn and improve.

Dekker, Sidney. The Field Guide to Understanding 'Human Error' (pp. 196-197). Ashgate Publishing Ltd. Kindle Edition. 

### Where to go from here

#### Examples of new view investigations

* TSB (2003). Aviation investigation report: In-flight fire leading to collision with water, Swissair Transport Limited, McDonnell Douglas MD-11 HB-IWF, Peggy’s Cove, Nova Scotia 5 nm SW, 2 September 1998. Gatineau, QC, Transportation Safety Board of Canada.
* Snook, S.A. (2000). Friendly fire: The accidental shootdown of US Black Hawks over Northern Iraq. Princeton, NJ: Princeton University Press.

#### Understanding complexity & drift into failure

* Barry Turner laid an important piece of conceptual groundwork for ideas
  around drift
* Charles Perrow explained how the potential for failure is a structural property of complex, interactive and tightly coupled systems.
* Diane Vaughan worked on understanding phenomenon of drift in complex system
  from point of view of people inside the org at the time

References: 

* Turner, B.A. (1978). Man-made disasters. London: Wykeham Publications.
* Perrow, C. (1984). Normal accidents: Living with high-risk technologies. New York: Basic Books.
* Vaughan, D. (1996). The Challenger launch decision: Risky technology, culture, and deviance at NASA. Chicago, IL: University of Chicago Press.
* Dekker, S.W.A. (2011). Drift into failure: From hunting broken components to understanding complex systems. Farnham, UK: Ashgate.

#### Safety differently and resilience

Safety as teh presence of positive capacities rather than absence of negative
events

* Hollnagel, E., Woods, D.D. et al. (2006). Resilience engineering: Concepts and precepts. Aldershot, UK: Ashgate.
* Weick, K.E. and Sutcliffe, K.M. (2007). Managing the unexpected: Resilient performance in an age of uncertainty. San Francisco, CA: Jossey-Bass.

Hollnagel's work on safety management:

* Hollnagel, E. (2009). The ETTO Principle: Efficiency-Thoroughness Trade-Off. Why things that go right sometimes go wrong. Aldershot, UK: Ashgate.
* Hollnagel, E. (2014). Safety I and Safety II: The past and future of safety management. Farnham, UK: Ashgate.

Accountability, learning and justice:

* Sharpe, V.A. (2004). Accountability: Patient safety and policy reform. Washington, DC: Georgetown University Press.
* Dekker, S.W.A. (2012). Just culture: Balancing safety and accountability (Second Ed.). Farnham, UK: Ashgate.
* Dekker, S.W.A. (2013). Second victim: Error, guilt, trauma and resilience. Boca Raton, FL: CRC Press/Taylor & Francis.



