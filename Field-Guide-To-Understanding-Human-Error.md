# Field Guide to Understanding 'Human Error' by Sidney Dekker

## Terms

human error, hindsight, bad apples, accountability, justice, hindsight,
safety, sharp end, blunt end, micro-matching, cherry-picking, shopping-bagging

## Table of contents

1. Two views of 'human error'
    * Bad people in safe systems, or well-intentioned people in imperfect
      systems?
    * People do not come to work to do a bad job
    * "Bad apples clearly exist"
    * "Accident-prone" workers
    * Individual differences exist
    * "There has to be some accountability"
    * "The authority-responsibility mismatch"
    * Accountability and the systems approach
    * New models of accountability
    * To report or not to report
    * Accountability and the "just culture"
2. Containing your reactions to failure
    * Retrospective
    * Counterfactual
    * Judgmental
    * Proximal
3. Doing a 'human error' investigation
    * Getting human factors data
    * Debriefing of participants
    * The aim of a debriefing
    * Dealing with disagreements and inconsistencies in briefings
    * Recordings, facts and analysis
    * Building a timeline
        - Two issues before you go on
        - Low-resolution communication timeline
        - Higher-resolution communication timeline
        - Conversation analysis
        - Connecting behavior and process
            * What was going on in the process at that time?
            * What other tasks would people have plausibly been involved in
              simultaneously, if at all?
        - How do you identify "events" in your data?
        - The "events" that were no events
        - Putting data in context
    * Putting data in context
        - Out of context I: micro-matching
            * a procedure or collection of rules
            * a set of cues
            * standards of good practice taht people's behavior falls short of
        - Imposing procedures onto history
        - Imposing available data onto history
        - Standrads of good practice
        - Out of context II: cherry-picking
        - Out of context III: the shopping bag



4. Explaining the patterns of breakdown
5. Understanding your accident model
6. Creating an effective safety department
7. Building a safety culture
8. Abandoning the fallacy of a quick fix
9. Epilogue: Speaking for the dead

## Aim of a debriefing

1. Have participants tell the story from their point of view, without
   presenting them with any replays or reminders that supposedly “refresh their
   memory” but would actually distort it.
2. Tell the story back to them as investigator. This is to check whether you
   understand the story as the participants understood it.
3. If you had not done so already, identify (together with participants) the
   critical junctures in a sequence of events.
4. Progressively probe and rebuild how the world looked to people on the inside
   of the situation at each juncture. Here it is appropriate to show a re-play
   (if available) to fill the gaps that may still exist, or to show the
   difference between data that were available to people and data that were
   actually observed by them.

At each juncture in sequence events, you want to get to know:

* Which cues were observed (what did he or she notice/see or did not notice what he or she had expected to notice?)
* What knowledge was used to deal with the situation? Did participants have any experience with similar situations that was useful in dealing with this one?
* What expectations did participants have about how things were going to develop, and what options did they think they have to influence the course of events?
* How did other influences (operational or organizational) help determine how they interpreted the situation and how they would act?

Cues:
* What were you seeing?
* What were you focusing on?
* What were you expecting to happen?

Interpretation: If you had to describe the situation to your colleague at that
point, what would you have told?

Errors: What mistakes (for examples in interpretation) were likely at this
point?

Previous experience/knowledge: 
* Were you reminded of any previous experience?
* Did this situation fit a standard scenario?
* Were you trained to deal with this situation?
* Were there any rules that applied clearly here?
* Did any other sources of knowledge suggest what to do?

Goals:
* What were you trying to achieve?
* Were there multiple goals at the same time?
* Was there time pressure or other limitations on what you could do?


## Callouts

Underneath every simple, obvious story about ‘human error,’ there is a deeper, more complex story about the organization.

If it made sense for people to do what they did, then it may make sense for others as well.

The New View does not claim that people are perfect. But it keeps you from judging and blaming people for not being perfect.

Practitioners are not all exposed to the same kind and level of accident risk. This makes it impossible to compare their accident rates and say that some, because of personal characteristics, are more accident-prone than others.

A “Bad Apple” problem, to the extent that you can prove its existence, is a system problem and a system responsibility.

There is no evidence that a system approach dilutes personal accountability. In fact, second victims show just how much responsibility practitioners take for things that go wrong.

Accountability can mean letting people tell their account, their story.

The challenge is to create a culture of accountability that encourages learning. Every step toward accountability that your organization takes should serve that goal. Every step that doesn’t serve that goal should be avoided.

If you truly want to create accountability and a “just culture” in your
organization, forget buying it off the shelf. It won’t work, independent of how
much you pay for it. You need to realize that it is going to cost you in
different ways than dollars.

The more you react to failure, the less you will understand it.

Hindsight gets you to oversimplify history. You will see events as simpler, more linear, and more predictable than they once were.

Bad process may still lead to good outcomes, and vice versa.

What you believe should have happened does not explain other people’s behavior. It just makes you look ignorant and arrogant.

To understand error, take the view from the inside of the tunnel and stop saying what people failed to do or should have done.


## Notes

Storytelling is a powerful mechanism for others to learn vicariously from trouble.

1. Don't ask who is responsible, ask what is responsible
2. Link knowledge of the messy details with the creation of justice
3. Explore the potential for restorative justice
4. Go from backward to forward-looking accountability
5. Put second victim support in place


Reactions to failure are typically:
* retrospective
* counterfactual
* judgmental
* proximal

The problem about taking this position of retrospective outsider is that it does not allow you to explain anything.

This means zooming out, away from looking just at the sharp end, and
incorporating blunt end policies and priorities and design choices and how
these help drive people’s goals and practices at the sharp end.

In order to understand other people’s assessments and actions, you must try to attain the perspective of the people who were there at the time.

## Important lessons (summarized in chapte r8)

Five sets of reminders about:

* your own org and nature of safety & risk in it
* what to think about when investigating 'human error'
* how to recognize Old View thinking
* how to create progress on safety with the New View

Whenever you try to understand 'human error', do not forget to take the point
of view of the person inside the situation

### Your org & 'human error'

1. Your org is not basically or inherently safe. People have to create safety
   by putting tools & tech to use while negotiating multiple systems goals at
   all levels of your org.
2. The priorities & prefs that people express through their practice may be
   logical reproduction of what entire org finds important
3. Human error is inevitable by-product of pursuit of success in imperfect,
   unstable, resource-constrained world. Occasional human contribution to
   failure occurs because complex systems need an overwhelming human
   contribution for their safety.
4. So 'human error' is never at the root of your safety problems. 'Human error'
   is the effect of trouble deeper inside your system.
5. It also means that 'human error' is not random. 'Human error' is
   systematically connected to features of people's tools, tasks and operating
   environment.


### What to think of when investigating 'human error'

* As far as the people involved were concerned, the otucome was not going to
  happen. Otherwise, they would have done somethign else.
* Nobody comes to work to do a bad job.
* Human error is not the cause of failure, but the effect.
* Explaining one error (e.g., operator error) by pointing to another (e.g., deficient management) does not explain anything.
* To understand what went on in somebody's mind, you have to reconstruct the
  situation in which the mind found itself.
* There is no such thing as the cause of a mishap. What you deem causal depends
  on your accident model.

### Doing something about your 'human error' problem

* No quick fixes
* Reprimanding bad apples accomplishes nothing
* Can't expect employees to be more committed to safety than managers are or
  appear to be
* Problems result from organization's complexity
* Do not expect that you can hold people accountable for their errors if you did not give them enough authority to live up to the responsibility you expect of them.


### Recognizing old view thinking

* Old View thinking sees ‘human error’ as the major threat to basically safe
  systems. Unreliable, erratic people undermine systems of multiple defenses,
  rules, procedures and other safeguards.
* Old View thinking will try to count and categorize errors, and endeavor to
  get the number of ‘human error’ incidents down. It assumes that safety, once
  established, can be maintained by monitoring and keeping people’s performance
  within pre-specified boundaries.
* Old View thinking will (unsuccessfully) try to revert to more automation,
  tighter procedures, closer supervision and reprimands to control erratic
  human performance.
* During downsizing, budget trimming and increased production pressures, Old
  View thinking will misinterpret ‘human errors’ as a source of trouble, when
  they are likely the inevitable downstream consequences of trying to do more
  with less.

### Creating progress on safety with the new view

* To create safety, you don’t need to rid your system of ‘human errors’.
  Instead, you need to realize how people at all levels in the organization
  contribute to the creation of safety and risk through goal trade-offs that
  are legitimate and desirable in their setting.
* Rather than trying to reduce “violations,” New View strategies will find out
  more about the gap between work-as-imagined and work-as-done—why it exists,
  what keeps it in place and how it relates to priorities among organizational
  goals (both stated and unstated).
* New View thinking wants to learn about authority–responsibility
  mismatches—places where you expect responsibility of your people, but where
  their situation is not giving them requisite authority to live up to that
  responsibility.
* You know your organization is maturing toward the New View once it actively
  tries to learn how it is learning about safety. This means your organization
  is calibrating whether its strategies for managing safety and risk are
  up-to-date.
* Every organization has room to improve its safety. What separates a strong
  safety culture from a weak one is not how large this room is. What matters is
  the organization’s willingness to explore this space, to find leverage points
  to learn and improve.

Dekker, Sidney. The Field Guide to Understanding 'Human Error' (pp. 196-197). Ashgate Publishing Ltd. Kindle Edition. 

### Where to go from here

#### Examples of new view investigations

* TSB (2003). Aviation investigation report: In-flight fire leading to collision with water, Swissair Transport Limited, McDonnell Douglas MD-11 HB-IWF, Peggy’s Cove, Nova Scotia 5 nm SW, 2 September 1998. Gatineau, QC, Transportation Safety Board of Canada.
* Snook, S.A. (2000). Friendly fire: The accidental shootdown of US Black Hawks over Northern Iraq. Princeton, NJ: Princeton University Press.

#### Understanding complexity & drift into failure

* Barry Turner laid an important piece of conceptual groundwork for ideas
  around drift
* Charles Perrow explained how the potential for failure is a structural property of complex, interactive and tightly coupled systems.
* Diane Vaughan worked on understanding phenomenon of drift in complex system
  from point of view of people inside the org at the time

References: 

* Turner, B.A. (1978). Man-made disasters. London: Wykeham Publications.
* Perrow, C. (1984). Normal accidents: Living with high-risk technologies. New York: Basic Books.
* Vaughan, D. (1996). The Challenger launch decision: Risky technology, culture, and deviance at NASA. Chicago, IL: University of Chicago Press.
* Dekker, S.W.A. (2011). Drift into failure: From hunting broken components to understanding complex systems. Farnham, UK: Ashgate.

#### Safety differently and resilience

Safety as teh presence of positive capacities rather than absence of negative
events

* Hollnagel, E., Woods, D.D. et al. (2006). Resilience engineering: Concepts and precepts. Aldershot, UK: Ashgate.
* Weick, K.E. and Sutcliffe, K.M. (2007). Managing the unexpected: Resilient performance in an age of uncertainty. San Francisco, CA: Jossey-Bass.

Hollnagel's work on safety management:

* Hollnagel, E. (2009). The ETTO Principle: Efficiency-Thoroughness Trade-Off. Why things that go right sometimes go wrong. Aldershot, UK: Ashgate.
* Hollnagel, E. (2014). Safety I and Safety II: The past and future of safety management. Farnham, UK: Ashgate.

Accountability, learning and justice:

* Sharpe, V.A. (2004). Accountability: Patient safety and policy reform. Washington, DC: Georgetown University Press.
* Dekker, S.W.A. (2012). Just culture: Balancing safety and accountability (Second Ed.). Farnham, UK: Ashgate.
* Dekker, S.W.A. (2013). Second victim: Error, guilt, trauma and resilience. Boca Raton, FL: CRC Press/Taylor & Francis.



