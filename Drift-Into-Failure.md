# Drift into Failure: From Hunting Broken Components to Understanding Complex Systems

Sidney, Dekker

## Terms

complexity, drift, failure, broken part, Newton-Descartes, diversity, systems
theory, unruly technology, theory, rational, rational choice theory,
decrementalism, relationships, high-reliability organizations, normalization of
deviance, complexity, efficiency, scarcity, competition, initial conditions,
normal, resilience

local rationality principle: people are doing what makes sense given the
situational indications, operational pressures, and organizational norms
existing at the time.

Systems thinking is about relationships, not parts

Five concepts that characterize drift:

* scarcity and competition
* Decrementalism, or small steps
* Sensitive dependence on initial conditions
* Unruly technology
* Contribution of the protective structure

Systems thinking perspcetive on the five concepts:

* Resource scarcity and competition, which leads to a chronic need to balance cost pressures with safety. In a complex system, this means that the thousands smaller and larger decisions and trade-offs that get made throughout the system each day can generate a joint preference without central coordination, and without apparent local consequences: production and efficiency get served in people’s local goal pursuits while safety gets sacrificed – but not visibly so;
* Decrementalism, where constant organizational and operational adaptation around goal conflicts and uncertainty produces small, stepwise normalization where each next decrement is only a small deviation from the previously accepted norm, and continued operational success is relied upon as a guarantee of future safety;
* Sensitive dependence on initial conditions. Because of the lack of a central designer or any part that knows the entire complex system, conditions can be changed in one of its corners for a very good reason and without any apparent implications: it’s simply no big deal. This may, however, generate reverberations through the interconnected webs of relationships; it can get amplified or suppressed as it modulates through the system;
* Unruly technology, which introduces and sustains uncertainties about how and when things may fail. Complexity can be a property of the technology-in-context. Even though parts or sub-systems can be modeled exhaustively in isolation (and therefore remain merely complicated), their operation with each other in a dynamic environment generates the unforeseeabilities and uncertainties of complexity;
* Contribution of the entire protective structure (the organization itself, but also the regulator, legislation, and other forms of oversight) that is set up and maintained to ensure safety (at least in principle: some regulators would stress that all they do is ensure regulatory compliance). Protective structures themselves can consist of complex webs of players and interactions, and are exposed to an environment that influences it with societal expectations, resource constraints, and goal interactions. This affects how it condones, regulates and helps rationalize or even legalizes definitions of “acceptable” system performance.


Banality of accidents thesis: incidents do not precede accidents. Normal work does.

Four ingredients of high reliability organizations:

1. Leadership safety objectives
2. The need for redundancy
3. Decentralization, culture and continuity.
4. Organizational learning (incremental learning through trial and error).

## Highlights

- drift occurs in small steps
- complex systems are sensitively dependent on initial conditions
- complex systems that can drift into failure are chaaracterized by unruly technology
- System thinking is about relationships, not parts
- Safety certification is about bridging the gap between a piece of gleaming new
    technology in the hand now, and its adapter, coevolved, grimy, greased-down
    wear and use further down the line.
- Incidents do not precede accidents. Normal work does.
- The so-called common-cause hypothesis (which holds that accidents and
    incidents have common causes and that incidents are qualitatively identical
    to accidents except for being just one step short of a true or complete
    failure) is probably wrong for complex systems.
- In high-reliability organizations, active searching and exploration for ways
    to do things more safely is preferred over passively adapting to regulation
    or top-down control
- Continuous operations and training, non-stop on-the-job education, a regular
   throughput of new students or other learners, and challenging operational
   workloads contribute greatly to reduced error rates and enhanced reliability.
- Smaller dangers are courted in order to understand and forestall larger ones.
- Instead [a belief about the possibility to continue operating safely] should be a belief that is open to intervention as as to keep it curious, open-minded, complexly sensitized, inviting of doubt, and ambivalent toward the past.
- [Reaching and staying at a high-reliability end-state] involves a
    preoccupation with failure, a reluctance to simplify, a sensitivity to
    operations, deference to expertise and a commitment to resilience.
- It also is an active consideration of all the places and moments where you
    don't want to fail.
- High-reliability theory suggests that it is this complexity of possible
    interpretations of events that allow organizaitons to better anticipate and
    detect what might go wrong in the future.
- These decisions are sound when set against local judgment criteria; given the
    time and budget pressures and short-term incentives that shape behavior.
    Given the knowledge, goals, and focus of attention by the decision-makers,
    as well as the nature of the data available to them at the time, it made
    sense.
- One of the ingredients in almost all stories of drift is a focus on production
    and efficiency.
- Recall Weick's and Perrow's warning: what cannot be believed cannot be seen.
- It is this insidious delegation, this handover, where the internalization of
    external pressure takes place.
- Instead, the processes by which such decisions come about, and by which
    decision-makers create their local rationality, are one key to understanding
    how safety can erode on the inside a complex socio-technical system.
- "normalization of deviance"
- The solution to risk, if any, is to ensure that the organization continually
    reflects critically on and challenges its own definition of "normal"
    operations, and finds ways to prioritize chronic safety concerns over acute
    production pressures.
- The idea that organizations are capable of inculcating a safety orientation
    among its members through recruitment, socializing and indoctrination is met
    with great skepticism.
- Strutural secrecy, with participants not knowing about what goes on in other
    parts of the organization, is a normal by-product of the bureaucratic
    organization and social nature of complex work.
- People do what locally makes sense to them, given their goals, knowledge and
    focus of attention in that settings.
- As with Vaughan's normalization of deviance, operational success with such
    adapted procedures is one of the strongest motivators for doing it again,
    and again.
- This, instead, can be achieved by making the boundaries of system
    performance explicit and known, and to help people develop skills at
    coping with the edges of those boundaries.
- Also, a reminder to try harder and watch out better, particularly during times
    of high workload, is a poor substitute for actually developing skills to
    cope at the boundary.
- Safety is an emergent property, and its erosion is not about the breakage or
    lack of quality of single components.
- Drifiting into failure is not so much
    about breakdowns or malfunctioning of components, as it is about an
    organization not adapting effectively to cope with the complexity of its own
    structure and environment.
- Organizational resilience is about finding the political, practical and
    operational means to invest in safety even under pressures of sacrcity and
    competition, because that may be when such investments are needed most.
- How can an organization become aware, and remain aware, of its models of risk
    and danger?
- Rather than being the result of a few or a number of component failures,
    accidents involve the unanticipated interaction fo a multitude of events in
    a complex system - events and interactions, often very normal, whose
    combinatorial explosion can quickly outwit people's best efforts at
    predicting and mitigating trouble.
- The idea behind system accidents is that our ability to intellectualy manage
    interactively complex systems has now been overtaken by our ability to build
    them and let them grow.
- All components can meet their specified design requirements, and still a
    failure can occur.
- The answer lies in understanding relationships.
- We never enumerated certainties, only possibilities. We never pinpointed, only
    sketched. That is what complexity and system thinking does.
- the behavior of the system cannot be reduced to the behavior of the
    constituent components, but only characterized on the basis of the multitude
    of ever-changing relationships between them.
- the performance of complex systems is typically optimized at the edge of
    chaos, just before system behavior will become unrecognizably turbulent.
- descriptions of complexity have to take history into account.
- Open systems mean that it can be quite difficult to define the border of a
    system.
- Complex systems operate under conditions far from equilibrium.
- in complex systems, history matters. Complex systems themselves have a
    history.
- Only constant training and indoctrination, as well as positive reinforcement,
    can help build confidence and acceptance for the use of unmitigated
    language.
- Decrementalism, where organizational and operational adaptation around goal
    conflicts and uncertainty produces small, stepwise normalization where each
    next decrement is only a small deviation from the previously accepted norm,
    and continued operational success is relied upon as a guarantee of future
    safety
- Resource scarcity and goal oppositions from one such pervasive influence.
- In fact, what is remarkable about this accident is that everybody was pretty
    much following the rules.
- Behavior that is locally rational, that responds to local conditions and makes
    sense given the various rules that govern it locally, can add up to
    profoundly irrational behavior at the system level.
- Just like a lot of other technical problems - NASA engineers were, and always
    had been, working in an environment where technical problems proliferated.
    Flying with flaws was the norm.
- ... safety problem into a maintenance problem. But what we really need to
    understand is how these conversions of language made sense to
    decision-makers at the time.
- the important question to ask ourselves is how organizations can be made aware
    early on that such shifts in language can have far-reaching consequences,
    even if those are hard to foresee
- Rather, we should be weary of renaming things that negotiate their perceived
    risk down from what it was before.
- Drift into failure, in these terms, is about optimizing the system until it is
    perched on that edge of chaos. There, in that critical state, big,
    devastating responses to small pertubations become possible.
- Remember the basic message of this book. The growth of complexity in society
    has got ahead of our understanding of how complex systems work and fail. Our
    technologies have got ahead of our theories. Our theoreis are still
    fundamentally reductionist, compoenential and lienar. Our technologies,
    however, are increaasingly complex, emergent and non-linear.
- Drift into failure involves the interation between diverse, interacting and
    adaptive entities whose micro-level behaviors produce macro-level patterns,
    to which they in turn adapt, creating new patterns.
- Investigations of past failures thus do not contain much predictive value for
    a complex system.
- In a complex system, an action controls almost nothing. But it influences
    almost everything.
- The commitment that is called for here is to see safety-critical organizations
    as complex adaptive systems.
- It is important to realize that, in complex systems, the effects of local
    decisions seldom stay local.
- The five features of  drift - scarcity and competition, small steps, sensitive
    dependency on initial conditions, unruly technology, and a contributing
    regulator.
- recommendations made by high-reliability theory include a validation fo
    minority opinion and an encouragement of dissent
- A local optimization may become a global disaster
- Outsiders might think about this very differently, and they may represent a
    resource that managers could capitalize
- But for a regulator it means learning an entirely new repetoire of languages
    and countermeasures: from componential, determinist, compliance to
    co/counter-evolving complexity
- in times of crisis, these people's responsibilities may well suddenly be
    affected by a part failure way outside their functional area.
- a Newtonian narrative of failure achieves its end only by erasing its true
    subject: human agency and the way it is configured in a hugely complex
    network of relationships and interdependencies.
- Complexity and systems thinking denies us the comfort of one objectively
    accessible reality that can, as long as we have accurate methods, arbitrate
    between what is true and what is false. Or between what is right and what is
    wrong.
- accuracy cannot be achieved in complex systems tsince they defy exhaustive description
- Authentic stories
- A story of a complex system is authentific if it succeeds in communicating
  some of the vitality of everyday life in that system
- In a complex system, there is no objective way to determine whose view is
  right and whose view is wrong, since the agents effectively live in different
  environments.
- Diversity of narrative can be seen as an enormous source of resilience in complex systems.
- In a complex system, we should gather as much information on the issue as possible
- In a post-Newtonian ehtic, there is no longer an obvious relationship between
    the behavior of parts in the system (or their malfunctioning ,for example,
    "human errors") and system-level outcomes. Instead, system-level behaviors
    emerge from the multitude of relationships, interdependencies and
    interconnections inside the system, but cannot be reduced to those
    relationships or interconnections. In a post-Newtonian ethic, we resist
    looking for the "causes" of failure or success. System-level outcomes have
    no clearly traceable causes as theire relationships to effects are neither
    simple nor linear.
- There is no objective way of constructing a story of what happened
- We can at best hope and aim to produce authentic stories, stories that are
    true to experience, true to the phenomenology of being there, of being
    suspend inside complexity.

## Outline

1. Failure is always an option
    - Who messed up here?
    - Rational choice theory
    - Technology has developed more quickly than theory
    - The Gaussian copula
    - Complexity, locality and rationality
    - Complexity and drift into failure
    - A great title, a lousy metaphor
    - Why we must not turn drift into the next folk model
2. Features of drift
    - The broken part
    - Unanswered questions
    - The outlines of drift
    - Scarcity and competition
    - Decrementalism, or small steps
    - Sensitive dependency on initial conditions
    - Unruly technology
    - Contribution of the protective structure
    - A story of drift
3. The legacy of Newton and Descartes
    - Why did Newton and Descrates have such an impact?
    - So why should we care?
    - What Newton can and cannot do
    - Columbia: parts that broke in sequence
    - Broken part, broken system?
    - Accidents come from relationships, not parts
    - We have Newton on a retainer
4. The search for the broken component
    - Broken components after a hailstorm
    - Broken ocmponents to explain a broken system
    - Newton and the simplicity of failure
    - Reductionism and the Eureka part
    - Causes for effects an be found
    - The foreseeability of harm
    - Time-reversibility
    - Completeness of knowledge
    - A Newtonian ethic of failure
5. Theorizing drift
    - Man-made disasters
    - The incubation and surprise of failure
    - Risk as energy to be contained: barrier analysis
    - High reliability organizations
    - Challenging the belief in continued safe operations
    - Goal interactions and production pressure
    - Normalizing deviance, structural secrecy and practical drift
    - The normalization of deviance
    - How to prevent the normalization of deviance
    - Structural secrecy and practical drift
    - Managing the information environment
    - Control theory and drift
    - Resilience engineering
6. What is complexity and systems thinking?
    - More redundancy and barriers, more complexity
    - Up and out, not down and in
    - Systems thinking
    - Complex systems theory
    - The butterfly effect
    - Adaptation and history
    - Complex versus complicated
    - Complexity and drift
    -
7. Manging the complexity of drift


## Failure is always an option

Features of complex systems

1. Complex systems can exhibit tendencies to drift into failure because of uncertainty and competition in their environment. Adaptation to these environmental features is driven by a chronic need to balance resource scarcity and cost pressures with safety.
2. Drift occurs in small steps.
3. Complex systems are sensitively dependent on initial conditions.
4. Complex systems that can drift into failure are characgerized by unruly technology.

## Features of drift

### Unanswered questions

Systems thinking is about relationships, not parts

System thinking is about the complexity of the whole, not the simplicity of
carved-out bits. Systems thinking is about non-linearity and dynamics, not about
linear cause-effect-cause sequences. Systems thinking is about accidents that
are more than the sum of the broken parts. It is about understanding how
accidents can happen when no parts are broken, or no parts are seen as broken.

### Outlines of drift

Five concepts that characterize drift:

- scarcity and competition
- Decrementalism, or small steps
- Sensitive dependence on initial conditions
- Unruly technology
- Contribution of the protective structure

### Scarcity and competition

This moved Langewiesche to say that Murphy’s law is wrong: everything that can
go wrong usually goes right, and then we draw the wrong conclusion.

### Sensitive dependency on initial conditions

Safety certification is about bridging the gap between a piece of gleaming new
technology in the hand now, and its adapted, coevolved, grimy, greased-down wear
and use further down the line.

### A Story of drift

This is the banality of accidents thesis. These are not incidents. Incidents do
not precede accidents. Normal work does. In these systems:

> accidents are different in nature from those occurring in safe systems: in this case accidents usually occur in the absence of any serious breakdown or even of any serious error. They result from a combination of factors, none of which can alone cause an accident, or even a serious incident; therefore these combinations remain difficult to detect and to recover using traditional safety analysis logic. For the same reason, reporting becomes less relevant in predicting major disasters.

